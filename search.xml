<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[dpvs源码分析（三）之网络层]]></title>
    <url>%2F2018%2F05%2F11%2Fdpvs%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%89%EF%BC%89%E4%B9%8B%E7%BD%91%E7%BB%9C%E5%B1%82%2F</url>
    <content type="text"><![CDATA[ipv4报文处理接dpvs源码分析（二）之链路层 我们知道在处理ipv4报文在下面的函数当中。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990static int ipv4_rcv(struct rte_mbuf *mbuf, struct netif_port *port)&#123; ... /*这个函数应该是参考了linux协议栈的处理。大概的意思是： *pskb_may_pull确保skb-&gt;data指向的内存包含的数据至少为IP头部大小，由于每个 *IP数据包包括IP分片必须包含一个完整的IP头部。如果小于IP头部大小，则缺失 *的部分将从数据分片中拷贝。这些分片保存在skb_shinfo(skb)-&gt;frags[]中。 */ //参考http://blog.chinaunix.net/uid-22577711-id-3220103.html if (mbuf_may_pull(mbuf, sizeof(struct ipv4_hdr)) != 0) goto inhdr_error; iph = ip4_hdr(mbuf); hlen = ip4_hdrlen(mbuf); if (((iph-&gt;version_ihl) &gt;&gt; 4) != 4 || hlen &lt; sizeof(struct ipv4_hdr)) goto inhdr_error; if (mbuf_may_pull(mbuf, hlen) != 0) goto inhdr_error; if (unlikely(!(port-&gt;flag &amp; NETIF_PORT_FLAG_RX_IP_CSUM_OFFLOAD))) &#123; if (unlikely(rte_raw_cksum(iph, hlen) != 0xFFFF)) goto csum_error; &#125; len = ntohs(iph-&gt;total_length); if (mbuf-&gt;pkt_len &lt; len) &#123; IP4_INC_STATS(intruncatedpkts); goto drop; &#125; else if (len &lt; hlen) goto inhdr_error; /* trim padding if needed */ if (mbuf-&gt;pkt_len &gt; len) &#123; if (rte_pktmbuf_trim(mbuf, mbuf-&gt;pkt_len - len) != 0) &#123; IP4_INC_STATS(indiscards); goto drop; &#125; &#125; mbuf-&gt;userdata = NULL; mbuf-&gt;l3_len = hlen;#ifdef CONFIG_DPVS_IPV4_DEBUG ip4_dump_hdr(iph, mbuf-&gt;port);#endif return INET_HOOK(INET_HOOK_PRE_ROUTING, mbuf, port, NULL, ipv4_rcv_fin);csum_error: IP4_INC_STATS(csumerrors);inhdr_error: IP4_INC_STATS(inhdrerrors);drop: rte_pktmbuf_free(mbuf); return EDPVS_INVPKT;&#125;int INET_HOOK(unsigned int hook, struct rte_mbuf *mbuf, struct netif_port *in, struct netif_port *out, int (*okfn)(struct rte_mbuf *mbuf))&#123; ... //执行hook函数 ops = list_entry(hook_list, struct inet_hook_ops, list); if (!list_empty(hook_list)) &#123; verdict = INET_ACCEPT; list_for_each_entry_continue(ops, hook_list, list) &#123;repeat: verdict = ops-&gt;hook(ops-&gt;priv, mbuf, &amp;state); if (verdict != INET_ACCEPT) &#123; if (verdict == INET_REPEAT) goto repeat; break; &#125; &#125; &#125; ... if (verdict == INET_ACCEPT || verdict == INET_STOP) &#123; return okfn(mbuf); &#125; else if (verdict == INET_DROP) &#123; rte_pktmbuf_free(mbuf); //继续协议栈处理 return EDPVS_DROP; &#125; else &#123; /* INET_STOLEN */ return EDPVS_OK; //交由下一个函数处理， &#125;&#125;]]></content>
      <categories>
        <category>网络转发</category>
      </categories>
      <tags>
        <tag>网络转发</tag>
        <tag>DPVS</tag>
        <tag>DPDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dpvs源码分析（二）之链路层]]></title>
    <url>%2F2018%2F05%2F11%2Fdpvs%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%BA%8C%EF%BC%89%E4%B9%8B%E9%93%BE%E8%B7%AF%E5%B1%82%2F</url>
    <content type="text"><![CDATA[本文仅仅概述dpvs二三层协议栈的处理流程，只会对重点函数和流程分析，以避免刚刚接触DPVS的同学被这些细节扰乱视听。后面的章节将会针对于某个某块进行重点分析。 接dpvs源码分析（一）之启动过程，我们知道，lcore_job_recv_fwd是首先被调动的函数：12345678910111213141516171819202122232425262728293031323334static void lcore_job_recv_fwd(void *arg)&#123; int i, j; portid_t pid; lcoreid_t cid; struct netif_queue_conf *qconf; cid = rte_lcore_id(); assert(LCORE_ID_ANY != cid); for (i = 0; i &lt; lcore_conf[lcore2index[cid]].nports; i++) &#123; pid = lcore_conf[lcore2index[cid]].pqs[i].id; assert(pid &lt; rte_eth_dev_count()); for (j = 0; j &lt; lcore_conf[lcore2index[cid]].pqs[i].nrxq; j++) &#123; qconf = &amp;lcore_conf[lcore2index[cid]].pqs[i].rxqs[j]; // 从arp_ring获取arp报文，最后调用lcore_process_packets 处理，所以直接看lcore_process_packets这个函数就好了。 lcore_process_arp_ring(qconf,cid); // 从网卡收包，存放于qconf-&gt;mbufs 结构体重，len为包的数量 qconf-&gt;len = netif_rx_burst(pid, qconf); //统计 lcore_stats_burst(&amp;lcore_stats[cid], qconf-&gt;len); //处理数据报文， //mbuf会在lcore_process_packets被释放 lcore_process_packets(qconf, qconf-&gt;mbufs, cid, qconf-&gt;len, 1); //将报文发送给Linux kernel kni_send2kern_loop(pid, qconf); &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182static void lcore_process_packets(struct netif_queue_conf *qconf, struct rte_mbuf **mbufs, lcoreid_t cid, uint16_t count, bool pretetch)&#123; ... /* prefetch packets 预先将数据包从内存加载到cache，这样有可能加快运行速度*/ if (pretetch) &#123; for (t = 0; t &lt; qconf-&gt;len &amp;&amp; t &lt; NETIF_PKT_PREFETCH_OFFSET; t++) rte_prefetch0(rte_pktmbuf_mtod(qconf-&gt;mbufs[t], void *)); &#125; /* L2 filter */ for (i = 0; i &lt; count; i++) &#123; ... /*校验mac地址，如果和物理设备的mac地址一样，被设置为RTE_TYPE_HOST。不一样则被设置为ETH_PKT_OTHERHOST*/ /* reuse mbuf.packet_type, it was RTE_PTYPE_XXX */ mbuf-&gt;packet_type = eth_type_parse(eth_hdr, dev); /* * 如果通过dpip命令设置了设备forward2kni on，那么所有的报文都会复制一份给kernel * 所有数据包复制一份通过kni发送给kernel， 原有mbuf不变。 */ if (dev-&gt;flag &amp; NETIF_PORT_FLAG_FORWARD2KNI) &#123; if (likely(NULL != (mbuf_copied = mbuf_copy(mbuf, pktmbuf_pool[dev-&gt;socket])))) kni_ingress(mbuf_copied, dev, qconf); else RTE_LOG(WARNING, NETIF, "%s: Failed to copy mbuf\n", __func__); &#125; /* * handle VLAN * if HW offload vlan strip, it's still need vlan module * to act as VLAN filter. * vlan_rcv会通过vlanid找到对应的dev，然后将dev id复制给mbuf-&gt;port_id */ if (eth_hdr-&gt;ether_type == htons(ETH_P_8021Q) || mbuf-&gt;ol_flags &amp; PKT_RX_VLAN_STRIPPED) &#123; if (vlan_rcv(mbuf, netif_port_get(mbuf-&gt;port)) != EDPVS_OK) &#123; rte_pktmbuf_free(mbuf); lcore_stats[cid].dropped++; continue; &#125; /*通过port id找到对应的dev设备*/ dev = netif_port_get(mbuf-&gt;port); if (unlikely(!dev)) &#123; rte_pktmbuf_free(mbuf); lcore_stats[cid].dropped++; continue; &#125; /*获取二层头*/ eth_hdr = rte_pktmbuf_mtod(mbuf, struct ether_hdr *); &#125; /* handler should free mbuf */ netif_deliver_mbuf(mbuf, eth_hdr-&gt;ether_type, dev, qconf, (dev-&gt;flag &amp; NETIF_PORT_FLAG_FORWARD2KNI) ? true:false, cid, pkts_from_ring); ... &#125;&#125;/*上文用到了vlan_rcv这里对vlan_rcv做解析*/int vlan_rcv(struct rte_mbuf *mbuf, struct netif_port *real_dev)&#123; ... // 剥离VLAN tag err = vlan_untag_mbuf(mbuf); if (unlikely(err != EDPVS_OK)) return err; // 依据VLAN tag找到对应的VLAN设备 dev = vlan_find_dev(real_dev, htons(ETH_P_8021Q), mbuf_vlan_tag_get_id(mbuf)); mbuf-&gt;port = dev-&gt;id; if (unlikely(mbuf-&gt;packet_type == ETH_PKT_OTHERHOST)) &#123; /*这里通过目的地址判断，包是不是发送给vlan的。*/ if (eth_addr_equal(&amp;ehdr-&gt;d_addr, &amp;dev-&gt;addr)) mbuf-&gt;packet_type = ETH_PKT_HOST/*如果是ETH_PKT_OTHERHOST报文会被丢弃*/; &#125; ...&#125; 前面对vlan神马的都处理了，接下来就是二层报文的处理了：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748static inline int netif_deliver_mbuf(struct rte_mbuf *mbuf, uint16_t eth_type, struct netif_port *dev, struct netif_queue_conf *qconf, bool forward2kni, lcoreid_t cid, bool pkts_from_ring)&#123; ... pkt_type注册见下文，这里通过eth_type获取ptk_type 其他协议，都会送给linux kernel pt = pkt_type_get(eth_type, dev); if (!forward2kni &amp;&amp; NULL == pt) &#123; /*通过kni，发送给linux kernel*/ kni_ingress(mbuf, dev, qconf); return EDPVS_OK; &#125; ... /*clone arp pkt to every queue*/ if (pt-&gt;type == rte_cpu_to_be_16(ETHER_TYPE_ARP) &amp;&amp; !pkts_from_ring/*arp_ring里的报文，肯定不能再入ring了。*/) &#123; /*将arp报文clone到每个队列，每个core维护自己的arp表现*/ ... &#125; /*Remove len bytes at the beginning of an mbuf. 移除二层头*/ if (unlikely(NULL == rte_pktmbuf_adj(mbuf, sizeof(struct ether_hdr)))) return EDPVS_INVPKT; /*在这里就开始处理上层协议了，目前只会处理ip和arp，也只注册了这两种*/ err = pt-&gt;func(mbuf, dev); if (err == EDPVS_KNICONTINUE) &#123; if (pkts_from_ring) &#123; /*pkt_from_ring为arp_ring过来的报文 * arp_ring过来的报文不再发给linux kernel，因此需要free mubf */ rte_pktmbuf_free(mbuf); return EDPVS_OK; &#125; if (!forward2kni &amp;&amp; likely(NULL != rte_pktmbuf_prepend(mbuf, (mbuf-&gt;data_off - data_off)))) // 发送给linux kernel kni_ingress(mbuf, dev, qconf); &#125; return EDPVS_OK;&#125; 不同类型的报文，注册不同的处理函数。1234567891011121314static struct pkt_type ip4_pkt_type = &#123; //.type = rte_cpu_to_be_16(ETHER_TYPE_IPv4), .func = ipv4_rcv, .port = NULL,&#125;;ip4_pkt_type.type = htons(ETHER_TYPE_IPv4);static struct pkt_type arp_pkt_type = &#123; //.type = rte_cpu_to_be_16(ETHER_TYPE_ARP), .func = neigh_resolve_input, .port = NULL,&#125;;arp_pkt_type.type = rte_cpu_to_be_16(ETHER_TYPE_ARP);目前只有ETHER_TYPE_ARP和ETHER_TYPE_IPv4被注册，也就是说dpvs协议栈目前仅仅会对这两种协议进行处理。其余的协议会通过kni传递个linux kernel。]]></content>
      <categories>
        <category>原创精选</category>
      </categories>
      <tags>
        <tag>网络转发</tag>
        <tag>DPVS</tag>
        <tag>DPDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于DPDK的多核令牌桶算法]]></title>
    <url>%2F2018%2F05%2F11%2F%E4%BB%A4%E7%89%8C%E6%A1%B6%E7%AE%97%E6%B3%95%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[QoS（Quality of Service，服务质量）指一个网络能够利用各种基础技术，为指定的网络通信提供更好的服务能力, 是网络的一种安全机制， 是用来解决网络延迟和阻塞等问题的一种技术。其中限速就是一种QOS机制，目前限速算法中用得比较多的是令牌桶算法。本文不打算套路目前有哪些限速算法，而主要套路基于DPDK的软件架构，如何实现多核限速算法，并获取到较高的性能。 令牌桶算法介绍简介所谓的令牌桶算法，顾名思义，就是向一个桶中按照一定的速率放入令牌，如下图所示。当有需要限速的数据包通过的时候，依据数据报文的长度算出需要取走的令牌数N。如果桶中的令牌数量大于N，那么此数据包就顺利通过了限速测试。反之，此数据包就没有通过限速测试，于是就将报文进行丢弃处理（大多数情况下是如此，也可以进行其他处理，依赖用户程序的设计）。整个令牌桶算法的主要原理就是这样，是不是感觉很简单，详细分析将在下文介绍。 基于DPDK的实现首先介绍下DPDK的两个函数，这也是在算法的实现中需要用到的两个函数，摘抄自DPDK官方文档：12345static uint64_t rte_get_timer_cycles(void )Get the number of cycles since boot from the default timer.static uint64_t rte_get_timer_hz(void)Get the number of cycles in one second for the default timer. 放入令牌在本设计中，以N = rte_get_timer_hz() 的速率向桶里面放令牌。也就是说，每秒放入一秒的时钟周期个数的令牌。的限速rate = 10000Bps，需要限速的包的大小为1500B。那么在处理该数据包，应该消耗多少令牌呢？利用小学数学应该可以算出，需要消耗1234如果一秒内有一个包：N * (1500 / 10000) &lt; N 此时可以通过如果一秒内有7个包N * (1500*7 / 10000) &gt; N 那么第七个包将会被丢弃 放入方式假设每秒应该放入N = rte_get_timer_hz() 个令牌 方式1： 设置定时器，在定时器的回调函数中放入令牌。 解析： 可以利用dpdk的定时器，比如设置每1秒，call以下回调函数，放入令牌1 * N个令牌。 缺陷1： 这种方式的问题在于，如果在这一秒钟内，如果前0.5秒的突发流量将令牌都消耗完了，那么后0.5秒的报文不就全部丢掉了。有人说，可以将这个间隔设置小一点啊，比入每1/1000秒放入N/1000个令牌。但是问题在于，这个间隔到底是多少好呢？ 缺陷2： 无法处理突发流量，原因见方式3。 方式2： 按时间间隔放入 解析： 按时间间隔放入，假设当前报文和上一个报文的时间间隔为1ms秒，那么放入1/1000 * N 个令牌。这种方式的好处在于针对于每个数据包的处理前都会放入令牌，分散了令牌的放入。 缺陷： 无法处理突发流量，原因见方式3。 方式3： 改进方式2 背景： 方式1和方式2都面临一个问题，如果有突发流量，那么可能在极短的时间内消耗全部的令牌，大量的流量被放行，从而对系统造成威胁。 解析： 如果当出现丢包，那么在接下来的200us内的前64个报文都会被丢弃。对于200us和64这个值都是经验值，可以在真实场景中进行调优。 缺陷： 这个方式降低了突发流量对系统的威胁，但是会对限速的准确性有一定的影响，通过测试可以证明。 方式2和方式3可以协作运行，比如用方式3先进行防攻击限速，然后再用方式2进行精确限速。 多线程处理 多线程处理的基本思想，如下图所示： 一个全局令牌桶 随着时间的推移，每次放入令牌仅仅放入全局令牌桶。为什么不放入线程令牌桶？假设需要限速的报文到了不同的线程，并且每个线程流量不同，那么对于每个线程放入令牌的数量就不得而知了。 每个线程拥有独立的子令牌桶 子令牌桶只消费令牌，不放入令牌。当子令牌桶的令牌不够时，从全局令牌桶索取令牌。 详细流程 下面将用流程图的形式来描述多线程令牌桶的实现。在最后会介绍这种令牌桶算法可能存在的问题。 测试结果设置限速11Mbps，测试结果为：]]></content>
      <categories>
        <category>原创精选</category>
      </categories>
      <tags>
        <tag>网络转发</tag>
        <tag>DPDK</tag>
        <tag>令牌桶算法</tag>
        <tag>多核限速</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c语言构造函数]]></title>
    <url>%2F2018%2F05%2F11%2Fc%E8%AF%AD%E8%A8%80%E6%9E%84%E9%80%A0%E5%87%BD%E6%95%B0%2F</url>
    <content type="text"><![CDATA[说起这个属性，要从fio的vpp说起，引擎的注册函数fio_libaio_register与反注册函数fio_libaio_unregister都没有其他函数调用，而fio又没有以动态库的形式将这两个函数供别的地方使用,但是这两个函数有宏定义fio_init和fio_exit来修饰。这个两个个宏定义为：12#define fio_init __attribute__((constructor)) #define fio_exit __attribute__((destructor)) 以如下构造函数为例,这是vpp中的代码： 12345678910111213#define VLIB_DECLARE_INIT_FUNCTION(x, tag) \vlib_init_function_t * _VLIB_INIT_FUNCTION_SYMBOL (x, tag) = x; \static void __vlib_add_##tag##_function_##x (void) \ __attribute__((__constructor__)) ; \static void __vlib_add_##tag##_function_##x (void) \&#123; \ vlib_main_t * vm = vlib_get_main(); \ static _vlib_init_function_list_elt_t _vlib_init_function; \ _vlib_init_function.next_init_function \ = vm-&gt;tag##_function_registrations; \ vm-&gt;tag##_function_registrations = &amp;_vlib_init_function; \ _vlib_init_function.f = &amp;x; \&#125; 这个两个属性是gcc提供的属性，在dpdk中也有体现。若函数被设定为constructor属性，则该函数会在main（）函数执行之前被自动的执行。若函数被设定为destructor属性，则该函数会在main（）函数执行之后或者exit（）被调用后被自动的执行。通过如下测试代码，能更加清晰地认识到这两个属性的作用：123456789101112131415#include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; void __attribute__((constructor)) con_func() &#123; printf("befor main: constructor is called..\n"); &#125; void __attribute__((destructor)) des_func() &#123; printf("after main: destructor is called..\n"); &#125; int main() &#123; printf("main func..\n"); return 0; &#125;结果：123befor main: constructor is called.. main func.. after main: destructor is called..]]></content>
      <categories>
        <category>原创精选</category>
      </categories>
      <tags>
        <tag>c语言构造</tag>
        <tag>c语言析构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dpvs源码分析（一）之启动过程]]></title>
    <url>%2F2018%2F05%2F11%2Fdpvs%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90%EF%BC%88%E4%B8%80%EF%BC%89%E4%B9%8B%E5%90%AF%E5%8A%A8%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[本文用于分析dpvs的启动流程，会对主要逻辑进行解析，忽略了一些边缘代码，比如配置文件解析，函数指针的注册等等。在阅读主逻辑的时候，如果有疑问的地方，再去看一些配置相关，初始化相关的代码。这样不仅高效而且不会那么枯燥。被忽略的代码将在本文中用…代替。 从main函数开始在src/mian.c文件中，main函数还是比较清晰的。首先是初始 -&gt; 然后启动端口 -&gt; 然后启动工作线程 -&gt; 主线程进入循环。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115int main(int argc, char *argv[])&#123; ... //各种初始化，暂不关心，用到的时候再看。 err = rte_eal_init(argc, argv); if (err &lt; 0) rte_exit(EXIT_FAILURE, "Invalid EAL parameters\n"); argc -= err, argv += err; rte_timer_subsystem_init(); if ((err = cfgfile_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail init configuration file: %s\n", dpvs_strerror(err)); if ((err = netif_virtual_devices_add()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail add virtual devices:%s\n", dpvs_strerror(err)); if ((err = dpvs_timer_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail init timer on %s\n", dpvs_strerror(err)); if ((err = tc_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail to init traffic control: %s\n", dpvs_strerror(err)); if ((err = netif_init(NULL)) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail to init netif: %s\n", dpvs_strerror(err)); /* Default lcore conf and port conf are used and may be changed here * with "netif_port_conf_update" and "netif_lcore_conf_set" */ if ((err = ctrl_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail to init ctrl plane: %s\n", dpvs_strerror(err)); if ((err = tc_ctrl_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail to init tc control plane: %s\n", dpvs_strerror(err)); if ((err = vlan_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail to init vlan: %s\n", dpvs_strerror(err)); if ((err = inet_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail to init inet: %s\n", dpvs_strerror(err)); if ((err = sa_pool_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail to init sa_pool: %s\n", dpvs_strerror(err)); if ((err = dp_vs_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail to init ipvs: %s\n", dpvs_strerror(err)); if ((err = netif_ctrl_init()) != EDPVS_OK) rte_exit(EXIT_FAILURE, "Fail to init netif_ctrl: %s\n", dpvs_strerror(err)); /* config and start all available dpdk ports */ nports = rte_eth_dev_count(); for (pid = 0; pid &lt; nports; pid++) &#123; dev = netif_port_get(pid); if (!dev) &#123; RTE_LOG(WARNING, DPVS, "port %d not found\n", pid); continue; &#125; //启动端口，配置端口。比如端口，队列，cpu的绑定等等。 err = netif_port_start(dev); if (err != EDPVS_OK) RTE_LOG(WARNING, DPVS, "Start %s failed, skipping ...\n", dev-&gt;name); &#125; /* print port-queue-lcore relation */ netif_print_lcore_conf(pql_conf_buf, &amp;pql_conf_buf_len, true, 0); RTE_LOG(INFO, DPVS, "\nport-queue-lcore relation array: \n%s\n", pql_conf_buf); /* start data plane threads */ netif_lcore_start(); //这里就是干活的线程，通过跟踪这个函数，最后会调用到netif_loop，即工作线程的loop /* write pid file */ if (!pidfile_write(DPVS_PIDFILE, getpid())) goto end; timer_sched_loop_interval = dpvs_timer_sched_interval_get(); assert(timer_sched_loop_interval &gt; 0); dpvs_state_set(DPVS_STATE_NORMAL); /* start control plane thread */ // 主线程循环，用于处理ctrl plane等消息，这里占不讨论，后续文章再讨论。 while (1) &#123; /* reload configuations if reload flag is set */ try_reload(); /* IPC loop */ sockopt_ctl(NULL); /* msg loop */ msg_master_process(); /* timer */ loop_cnt++; if (loop_cnt % timer_sched_loop_interval == 0) rte_timer_manage(); /* kni */ kni_process_on_master(); /* process mac ring on master */ neigh_process_ring(NULL); /* increase loop counts */ netif_update_master_loop_cnt(); &#125; ... exit(0);&#125; DPVS dataplane线程，即工作线程DPVS在netif.c文件中static int netif_loop(void * dummy) 函数中收取，处理和发送数据包。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273static int netif_loop(void *dummy)&#123; ... try_isol_rxq_lcore_loop(); if (0 == lcore_conf[lcore2index[cid]].nports) &#123; RTE_LOG(INFO, NETIF, "[%s] Lcore %d has nothing to do.\n", __func__, cid); return EDPVS_IDLE; &#125; /* 这里收包和处理CPU分离，是指收包的CPU和处理包的CPU不同，应该是为了增加网卡的吞吐能力吧。 只有在/etc/dpvs.conf配置文件中配置了isol_rx_cpu_ids才会生效，暂时用不到，在后文中再来分析吧*/ list_for_each_entry(job, &amp;netif_lcore_jobs[NETIF_LCORE_JOB_INIT], list) &#123; do_lcore_job(job); &#125; /* NETIF_LCORE_JOB_INIT type类型暂时没有用到，忽略之*/ while (1) &#123;#ifdef CONFIG_RECORD_BIG_LOOP loop_start = rte_get_timer_cycles();#endif/* CONFIG_RECORD_BIG_LOOP给统计和debug用，暂不关心 */ lcore_stats[cid].lcore_loop++; list_for_each_entry(job, &amp;netif_lcore_jobs[NETIF_LCORE_JOB_LOOP], list) &#123; do_lcore_job(job); &#125; ++netif_loop_tick[cid]; list_for_each_entry(job, &amp;netif_lcore_jobs[NETIF_LCORE_JOB_SLOW], list) &#123; if (netif_loop_tick[cid] % job-&gt;skip_loops == 0) &#123; do_lcore_job(job); //netif_loop_tick[cid] = 0; &#125; &#125; /* 上面代码有点类似于netfilter处理链，数据包会被处理链的job依次处理, 这些job链放在netif_lcore_jobs 这个全局变量中。 那么，有哪些job呢，在什么地方初始化这个全局变量的呢？他们的处理顺序是怎么样的？在下一节会有说明*/#ifdef CONFIG_RECORD_BIG_LOOP loop_end = rte_get_timer_cycles(); loop_time = (loop_end - loop_start) * 1E6 / cycles_per_sec; if (loop_time &gt; longest_lcore_loop[cid]) &#123; RTE_LOG(WARNING, NETIF, "update longest_lcore_loop[%d] = %d (&lt;- %d)\n", cid, loop_time, longest_lcore_loop[cid]); longest_lcore_loop[cid] = loop_time; &#125; if (loop_time &gt; BIG_LOOP_THRESH) &#123; print_job_time(buf, sizeof(buf)); RTE_LOG(WARNING, NETIF, "lcore[%d] loop over %d usecs (actual=%d, max=%d):\n%s\n", cid, BIG_LOOP_THRESH, loop_time, longest_lcore_loop[cid], buf); &#125;#endif/* CONFIG_RECORD_BIG_LOOP给统计和debug用，暂不关心 */ &#125; return EDPVS_OK;&#125;//那么do_lcore_job 做了什么呢？ 其实do_lcore_job就是调用了job结构体中注册的函数指针指向的函数。static inline void do_lcore_job(struct netif_lcore_loop_job *job)&#123;#ifdef CONFIG_RECORD_BIG_LOOP uint64_t job_start, job_end; job_start = rte_get_timer_cycles();#endif job-&gt;func(job-&gt;data); // 这里就是真正干活的地方了。 // func 函数指针在main函数中进行了初始化，初始化过程及代码位置在下一节会讲到。#ifdef CONFIG_RECORD_BIG_LOOP job_end = rte_get_timer_cycles(); job-&gt;job_time[rte_lcore_id()] = (job_end - job_start) * 1E6 / cycles_per_sec;#endif&#125; DPVS dataplane线程job初始化（ 也就是 netif_lcore_jobs 全局变量的初始化）目前job的类型为，又有NETIF_LCORE_JOB_LOOP和NETIF_LCORE_JOB_SLOW有用到：123456enum netif_lcore_job_type &#123; NETIF_LCORE_JOB_INIT = 0, NETIF_LCORE_JOB_LOOP = 1, NETIF_LCORE_JOB_SLOW = 2, NETIF_LCORE_JOB_TYPE_MAX = 3,&#125;; job注册函数netif_lcore_loop_job_register 函数将job这测到netif_lcore_jobs这个全局变量中。12345678910111213141516171819int netif_lcore_loop_job_register(struct netif_lcore_loop_job *lcore_job)&#123; struct netif_lcore_loop_job *cur; if (unlikely(NULL == lcore_job)) return EDPVS_INVAL; list_for_each_entry(cur, &amp;netif_lcore_jobs[lcore_job-&gt;type], list) &#123; if (cur == lcore_job) &#123; return EDPVS_EXIST; &#125; &#125; if (unlikely(NETIF_LCORE_JOB_SLOW == lcore_job-&gt;type &amp;&amp; lcore_job-&gt;skip_loops &lt;= 0)) return EDPVS_INVAL; list_add_tail(&amp;lcore_job-&gt;list, &amp;netif_lcore_jobs[lcore_job-&gt;type]); //netif_lcore_jobs 记录job的全局变量，这个在netif_loop用到了 return EDPVS_OK;&#125; 那么在哪些位置调用了netif_lcore_loop_job_register 注册job，通过阅读源代码，可以发现注册NETIF_LCORE_JOB_LOOP， NETIF_LCORE_JOB_SLOW 这两种类型的job分布在如下所示位置。 NETIF_LCORE_JOB_LOOP job注册 第一处： main-&gt;netif_init-&gt;netif_lcore_init函数中：12345678910111213/* register lcore jobs*/snprintf(netif_jobs[0].name, sizeof(netif_jobs[0].name) - 1, "%s", "recv_fwd");netif_jobs[0].func = lcore_job_recv_fwd;netif_jobs[0].data = NULL;netif_jobs[0].type = NETIF_LCORE_JOB_LOOP;snprintf(netif_jobs[1].name, sizeof(netif_jobs[1].name) - 1, "%s", "xmit");netif_jobs[1].func = lcore_job_xmit;netif_jobs[1].data = NULL;netif_jobs[1].type = NETIF_LCORE_JOB_LOOP;snprintf(netif_jobs[2].name, sizeof(netif_jobs[2].name) - 1, "%s", "timer_manage");netif_jobs[2].func = lcore_job_timer_manage;netif_jobs[2].data = NULL;netif_jobs[2].type = NETIF_LCORE_JOB_LOOP; 第二处： main-&gt;ctrl_init-&gt;msg_init1234567ctrl_lcore_job.func = slave_lcore_loop_func;ctrl_lcore_job.data = NULL;ctrl_lcore_job.type = NETIF_LCORE_JOB_LOOP;if ((ret = netif_lcore_loop_job_register(&amp;ctrl_lcore_job)) &lt; 0) &#123; RTE_LOG(ERR, MSGMGR, "%s: fail to register ctrl func on slave lcores\n", __func__); return ret;&#125; NETIF_LCORE_JOB_SLOW job注册 第一处： main-&gt;inet_init -&gt; ipv4_init-&gt; ipv4_frag_init 12345frag_job.func = ipv4_frag_job;frag_job.data = NULL;frag_job.type = NETIF_LCORE_JOB_SLOW;frag_job.skip_loops = IP4_FRAG_FREE_DEATH_ROW_INTERVAL;err = netif_lcore_loop_job_register(&amp;frag_job); 第二处： mian-&gt;inet_init -&gt; neigh_init -&gt; arp_init 12345neigh_sync_job.func = neigh_process_ring;neigh_sync_job.data = NULL;neigh_sync_job.type = NETIF_LCORE_JOB_SLOW;neigh_sync_job.skip_loops = NEIGH_PROCESS_MAC_RING_INTERVAL;err = netif_lcore_loop_job_register(&amp;neigh_sync_job); 通过上述分析，那么我们可以知道job的处理流程为 以下流程虽然都要执行，但是函数前后并不是强制依赖，比如lcore_job_timer_manage 不依赖于lcore_job_xmit的执行结果12lcore_job_recv_fwd -&gt; lcore_job_xmit -&gt; lcore_job_timer_manage -&gt; slave_lcore_loop_func -&gt;ipv4_frag_job -&gt; neigh_process_ring 这些函数的具体功能，将在下一章节进行分析。 启动过程就到此结束了，若有疑问，欢迎发邮件和我联系。]]></content>
      <categories>
        <category>网络转发</category>
      </categories>
      <tags>
        <tag>网络转发</tag>
        <tag>DPVS</tag>
        <tag>DPDK</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[container_of]]></title>
    <url>%2F2018%2F05%2F11%2Fcontainer_of%2F</url>
    <content type="text"><![CDATA[linux 内核链表的实现有段代码比较有意思，这里记录一下。 背景当在阅读遍历链表这个宏定义时，可能会对container_of有些许疑惑。12345678910111213141516171819202122232425262728293031323334353637/** * list_for_each_entry - iterate over list of given type * @pos: the type * to use as a loop cursor. * @head: the head for your list. * @member: the name of the list_head within the struct. */ #define list_for_each_entry(pos, head, member) \ for (pos = list_first_entry(head, typeof(*pos), member); \ &amp;pos-&gt;member != (head); \ pos = list_next_entry(pos, member)) /** * list_entry - get the struct for this entry * @ptr: the &amp;struct list_head pointer. * @type: the type of the struct this is embedded in. * @member: the name of the list_head within the struct. */#define list_entry(ptr, type, member) \ container_of(ptr, type, member)/** * list_first_entry - get the first element from a list * @ptr: the list head to take the element from. * @type: the type of the struct this is embedded in. * @member: the name of the list_head within the struct. * * Note, that list is expected to be not empty. */#define list_first_entry(ptr, type, member) \ list_entry((ptr)-&gt;next, type, member)#ifndef container_of#define container_of(ptr, type, member) \ (type *)((char *)(ptr) - (char *) &amp;((type *)0)-&gt;member)#endif 解析123#define container_of(ptr, type, member) \ (type *)((char *)(ptr) - (char *) &amp;((type *)0)-&gt;member)#endif 如上定义，为了更直观，定义如下数据结构：12345678910111213struct list_head &#123; struct list_head *next; struct list_head *prev; &#125;;struct type&#123; int a; int b; struct list_head list; int c;&#125;;那么对于container_of宏定义，三个参数分别对应： ptr: list的内存地址 type: list所在数据结构的类型 member： 是指list_head定义变量的名字，那么在这里就叫list 这个宏定义的功能：通过member的地址和member的名字获取member所在数据结构的首地址。首先： (char ) &amp;((type )0)-&gt;member获取member在type类型中的偏移量然后： 用ptr减去member在type类型数据结构中的偏移量，那么就得到了member所在type变量的首地址。 最后举例：123456789101112131415161718192021222324252627#include&lt;stdio.h&gt;struct A&#123; int a; int b; int c;&#125;;int main(void)&#123; struct A x; struct A *y; x.a = 1024; printf("output1: %0x\n", &amp;(((struct A*)0)-&gt;b)); y = (struct A*) (((char*)(&amp;x.b)) - ((char*)&amp;(((struct A*)0)-&gt;b))); printf("output2: %d\n", y-&gt;a); printf("output3:%0x\n", &amp;x); printf("output4:%0x\n", y); return 0;&#125;运行结果为：output1: 4output2: 1024output3:5b22a1e0output4:5b22a1e0]]></content>
      <categories>
        <category>原创精选</category>
      </categories>
      <tags>
        <tag>container_of</tag>
        <tag>linux</tag>
        <tag>链表</tag>
      </tags>
  </entry>
</search>
